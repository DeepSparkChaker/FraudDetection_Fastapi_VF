{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the librarys\n",
    "import pandas as pd #To work with dataset\n",
    "import numpy as np #Math library\n",
    "import matplotlib.gridspec as gridspec\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly import tools\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "import seaborn as sns #Graph library that use matplot in background\n",
    "import matplotlib.pyplot as plt #to plot some parameters in seaborn\n",
    "# Preparation  \n",
    "#import category_encoders as ce\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler,Normalizer,RobustScaler,MaxAbsScaler,MinMaxScaler,QuantileTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "# Import StandardScaler from scikit-learn\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer,IterativeImputer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import make_column_transformer,ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline,FeatureUnion\n",
    "from sklearn.manifold import TSNE\n",
    "# Import train_test_split()\n",
    "# Metrics\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import make_scorer,precision_score,recall_score,f1_score\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.metrics import roc_curve,confusion_matrix,classification_report\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import datetime, date\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC,LinearRegression\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "#import tensorflow as tf \n",
    "#from tensorflow.keras import layers\n",
    "#from tensorflow.keras.callbacks import EarlyStopping\n",
    "#from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "#import smogn\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier,HistGradientBoostingRegressor\n",
    "# For training random forest model\n",
    "import lightgbm as lgb\n",
    "from scipy import sparse\n",
    "from sklearn.neighbors import KNeighborsRegressor \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans \n",
    "# Model selection\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression,f_classif,chi2\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import mutual_info_classif,VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from lightgbm import LGBMClassifier,LGBMRegressor\n",
    "import lightgbm as lgbm\n",
    "#from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "#from xgboost import XGBClassifier,XGBRegressor\n",
    "from sklearn import set_config\n",
    "from itertools import combinations\n",
    "import category_encoders as ce\n",
    "#import smong \n",
    "import os \n",
    "import warnings\n",
    "# import optuna \n",
    "from joblib import Parallel, delayed\n",
    "import joblib \n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from sklearn import set_config\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Optional, Union\n",
    "set_config(display='diagram')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Model config in json format\"\"\"\n",
    "cfg = {\n",
    "    \"data\": {\n",
    "        ##alldata\n",
    "        \"path\": \"C:/Users/rzouga/Desktop/ALLINHERE/ALLINHERE/FraudDetection/transactions_train.csv\"\n",
    "        # small sample:\n",
    "       # \"path\": \"C:/Users/rzouga/Desktop/ALLINHERE/ALLINHERE/FraudDetection/X_train_transactions_train.csv\"\n",
    "    },\n",
    "    # \"data_test\": {\n",
    "    #   \"path\": \"../input/ventilator-pressure-prediction/test.csv\"\n",
    "    # },\n",
    "    # \"data_submission\": {\n",
    "    #   \"path\": \"../input/ventilator-pressure-prediction/test.csv\"\n",
    "    # },\n",
    "    \"train\": {\n",
    "        'fit_params': {'early_stopping_rounds': 100, 'verbose': 55000},\n",
    "        'n_fold': 5,\n",
    "        'seeds': [2021],\n",
    "        'target_col': \"Fraud\",\n",
    "        'debug': False\n",
    "\n",
    "    },\n",
    "    \"model\": {'n_estimators': 11932, \n",
    "                    'max_depth': 16, \n",
    "                    'learning_rate': 0.005352340588475586,\n",
    "                    'lambda_l1': 1.4243404105489683e-06,\n",
    "                    'lambda_l2': 0.04777178032735788,\n",
    "                    'num_leaves': 141, \n",
    "                    'feature_fraction': 0.6657626611307914, \n",
    "                    'bagging_fraction': 0.9115997498937961,\n",
    "                    'bagging_freq': 1,\n",
    "                    'min_child_samples': 51,\n",
    "                     \"objective\": \"binary\",\n",
    "                     #\"metric\": \"binary_logloss\",\n",
    "                     \"verbosity\": -1,\n",
    "                     \"boosting_type\": \"gbdt\",\n",
    "                     #\"random_state\": 228,\n",
    "                     \"metric\": \"auc\",\n",
    "                     #\"device\": \"gpu\",\n",
    "                     'tree_method': \"gpu_hist\"\n",
    "                    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    \"\"\"save log\"\"\"\n",
    "    def __init__(self, path):\n",
    "        self.general_logger = logging.getLogger(path)\n",
    "        stream_handler = logging.StreamHandler()\n",
    "        file_general_handler = logging.FileHandler(os.path.join(path, 'Experiment.log'))\n",
    "        if len(self.general_logger.handlers) == 0:\n",
    "            self.general_logger.addHandler(stream_handler)\n",
    "            self.general_logger.addHandler(file_general_handler)\n",
    "            self.general_logger.setLevel(logging.INFO)\n",
    "\n",
    "    def info(self, message):\n",
    "        # display time\n",
    "        self.general_logger.info('[{}] - {}'.format(self.now_string(), message))\n",
    "\n",
    "    @staticmethod\n",
    "    def now_string():\n",
    "        return str(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    \n",
    "class Util:\n",
    "    \"\"\"save & load\"\"\"\n",
    "    @classmethod\n",
    "    def dump(cls, value, path):\n",
    "        joblib.dump(value, path, compress=True)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        return joblib.load(path)\n",
    "        \n",
    "class HorizontalDisplay:\n",
    "    \"\"\"display dataframe\"\"\"\n",
    "    def __init__(self, *args):\n",
    "        self.args = args\n",
    "\n",
    "    def _repr_html_(self):\n",
    "        template = '<div style=\"float: left; padding: 10px;\">{0}</div>'\n",
    "        return \"\\n\".join(template.format(arg._repr_html_())\n",
    "                         for arg in self.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = \"../input/ventilator-pressure-prediction\"\n",
    "EXP = \"./\"\n",
    "EXP_MODEL = os.path.join(EXP, \"model\")\n",
    "EXP_FIG = os.path.join(EXP, \"fig\")\n",
    "EXP_PREDS = os.path.join(EXP, \"preds\")\n",
    "\n",
    "# make dirs\n",
    "for d in [EXP_MODEL, EXP_FIG, EXP_PREDS]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# utils\n",
    "logger = Logger(EXP)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Config class\"\"\"\n",
    "import json\n",
    "from types import SimpleNamespace\n",
    "class Config:\n",
    "    name_v1 = \"lgb baseline\"\n",
    "    \"\"\"Config class which contains data, train and model hyperparameters\"\"\"\n",
    "    def __init__(self, data, train, model):\n",
    "        self.data = data\n",
    "        self.train = train\n",
    "        self.model = model\n",
    "    @classmethod\n",
    "    def from_json(cls, cfg):\n",
    "        \"\"\"Creates config from json\"\"\"\n",
    "        params = json.loads(json.dumps(cfg), object_hook=lambda d: SimpleNamespace(**d))\n",
    "        return cls(params.data, params.train, params.model)\n",
    "\n",
    "class HelperObject(object):\n",
    "    \"\"\"Helper class to convert json into Python object\"\"\"\n",
    "    def __init__(self, dict_):\n",
    "        self.__dict__.update(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Data Loader\"\"\"\n",
    "class DataLoader:\n",
    "    \"\"\"Data Loader class\"\"\"\n",
    "    @staticmethod\n",
    "    def load_data(data_config):\n",
    "        \"\"\"Loads dataset from path\"\"\"\n",
    "        return pd.read_csv(data_config.path)\n",
    "    \n",
    "%time\n",
    "if __name__ == \"__main__\":\n",
    "    train = DataLoader().load_data(Config.from_json(cfg).data)\n",
    "    print(train.head())\n",
    "    print('shape of data {}'.format(train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnsSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, positions):\n",
    "        self.positions = positions\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #return np.array(X)[:, self.positions]\n",
    "        return X.loc[:, self.positions] \n",
    "########################################################################\n",
    "class CustomLogTransformer(BaseEstimator, TransformerMixin):\n",
    "    # https://towardsdatascience.com/how-to-write-powerful-code-others-admire-with-custom-sklearn-transformers-34bc9087fdd\n",
    "    def __init__(self):\n",
    "        self._estimator = PowerTransformer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_copy = np.copy(X) + 1\n",
    "        self._estimator.fit(X_copy)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = np.copy(X) + 1\n",
    "\n",
    "        return self._estimator.transform(X_copy)\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        X_reversed = self._estimator.inverse_transform(np.copy(X))\n",
    "\n",
    "        return X_reversed - 1  \n",
    "\n",
    "class TemporalVariableTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Temporal elapsed time transformer\n",
    "\n",
    "    def __init__(self, variables, reference_variable):\n",
    "        \n",
    "        if not isinstance(variables, list):\n",
    "            raise ValueError('variables should be a list')\n",
    "        \n",
    "        self.variables = variables\n",
    "        self.reference_variable = reference_variable\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # we need this step to fit the sklearn pipeline\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "       # so that we do not over-write the original dataframe\n",
    "        X = X.copy()\n",
    "        \n",
    "        for feature in self.variables:\n",
    "            X[feature] = X[self.reference_variable] - X[feature]\n",
    "\n",
    "        return X\n",
    "class CustomImputer(BaseEstimator, TransformerMixin) : \n",
    "    def __init__(self, variable, by) : \n",
    "            #self.something enables you to include the passed parameters\n",
    "            #as object attributes and use it in other methods of the class\n",
    "            self.variable = variable\n",
    "            self.by = by\n",
    "\n",
    "    def fit(self, X, y=None) : \n",
    "        self.map = X.groupby(self.by)[variable].mean()\n",
    "        #self.map become an attribute that is, the map of values to\n",
    "        #impute in function of index (corresponding table, like a dict)\n",
    "        return self\n",
    "\n",
    "def transform(self, X, y=None) : \n",
    "    X[variable] = X[variable].fillna(value = X[by].map(self.map))\n",
    "    #Change the variable column. If the value is missing, value should \n",
    "    #be replaced by the mapping of column \"by\" according to the map you\n",
    "    #created in fit method (self.map)\n",
    "    return X\n",
    "\n",
    "    # categorical missing value imputer\n",
    "class Mapper(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, variables, mappings):\n",
    "\n",
    "        if not isinstance(variables, list):\n",
    "            raise ValueError('variables should be a list')\n",
    "\n",
    "        self.variables = variables\n",
    "        self.mappings = mappings\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # we need the fit statement to accomodate the sklearn pipeline\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for feature in self.variables:\n",
    "            X[feature] = X[feature].map(self.mappings)\n",
    "\n",
    "        return X  \n",
    "    \n",
    "##########################################################################\n",
    "class CountFrequencyEncoder(BaseEstimator, TransformerMixin):\n",
    "    #temp = df['card1'].value_counts().to_dict()\n",
    "    #df['card1_counts'] = df['card1'].map(temp)\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoding_method: str = \"count\",\n",
    "        variables: Union[None, int, str, List[Union[str, int]]] = None,\n",
    "        keep_variable=True,\n",
    "                  ) -> None:\n",
    "\n",
    "        self.encoding_method = encoding_method\n",
    "        self.variables = variables\n",
    "        self.keep_variable=keep_variable\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):\n",
    "        \"\"\"\n",
    "        Learn the counts or frequencies which will be used to replace the categories.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: pandas dataframe of shape = [n_samples, n_features]\n",
    "            The training dataset. Can be the entire dataframe, not just the\n",
    "            variables to be transformed.\n",
    "        y: pandas Series, default = None\n",
    "            y is not needed in this encoder. You can pass y or None.\n",
    "        \"\"\"\n",
    "        self.encoder_dict_ = {}\n",
    "\n",
    "        # learn encoding maps\n",
    "        for var in self.variables:\n",
    "            if self.encoding_method == \"count\":\n",
    "                self.encoder_dict_[var] = X[var].value_counts().to_dict()\n",
    "\n",
    "            elif self.encoding_method == \"frequency\":\n",
    "                n_obs = float(len(X))\n",
    "                self.encoder_dict_[var] = (X[var].value_counts() / n_obs).to_dict()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        # replace categories by the learned parameters\n",
    "        X = X.copy()\n",
    "        for feature in self.encoder_dict_.keys():\n",
    "            if self.keep_variable:\n",
    "                X[feature+'_fq_enc'] = X[feature].map(self.encoder_dict_[feature])\n",
    "            else:\n",
    "                X[feature] = X[feature].map(self.encoder_dict_[feature])\n",
    "        return X[self.variables].to_numpy()\n",
    "#################################################   \n",
    "class FeaturesEngineerGroup(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,groupping_method =\"mean\",\n",
    "                   variables=  \"amount\",\n",
    "                   groupby_variables = \"nameOrig\"                         \n",
    "                 ) :\n",
    "        self.groupping_method = groupping_method\n",
    "        self.variables=variables\n",
    "        self.groupby_variables=groupby_variables\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Learn the mean or median of  amount of each client which will be used to create new feature for each unqiue client in order to undersatant thier behavior .\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: pandas dataframe of shape = [n_samples, n_features]\n",
    "        The training dataset. Can be the entire dataframe, not just the\n",
    "        variables to be transformed.\n",
    "        y: pandas Series, default = None\n",
    "        y is not needed in this encoder. You can pass y or None.\n",
    "        \"\"\"\n",
    "        self.group_amount_dict_ = {}\n",
    "        #df.groupby('card1')['TransactionAmt'].agg(['mean']).to_dict()\n",
    "        #temp = df.groupby('card1')['TransactionAmt'].agg(['mean']).rename({'mean':'TransactionAmt_card1_mean'},axis=1)\n",
    "        #df = pd.merge(df,temp,on='card1',how='left')\n",
    "        #target_mean = df_train.groupby(['id1', 'id2'])['target'].mean().rename('avg')\n",
    "        #df_test = df_test.join(target_mean, on=['id1', 'id2'])\n",
    "        #lifeExp_per_continent = gapminder.groupby('continent').lifeExp.mean()\n",
    "        # learn mean/medain \n",
    "        #for groupby in self.groupby_variables:\n",
    "         #   for var in self.variables:\n",
    "        if self.groupping_method == \"mean\":\n",
    "            self.group_amount_dict_[self.variables] =X.fillna(np.nan).groupby([self.groupby_variables])[self.variables].agg(['mean']).to_dict()\n",
    "        elif self.groupping_method == \"median\":\n",
    "            self.group_amount_dict_[self.variables] =X.fillna(np.nan).groupby([self.groupby_variables])[self.variables].agg(['median']).to_dict()\n",
    "        else:\n",
    "            print('error , chose mean or median')\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X = X.copy()\n",
    "        #for col in self.variables:\n",
    "         #   for agg_type in self.groupping_method:\n",
    "        new_col_name =  self.variables+'_Transaction_'+ self.groupping_method\n",
    "        X[new_col_name] = X[self.groupby_variables].map(self.group_amount_dict_[ self.variables][self.groupping_method])\n",
    "        return X[new_col_name].to_numpy().reshape(-1,1)    \n",
    "    \n",
    "################################################   \n",
    "class FeaturesEngineerGroup2(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,groupping_method =\"mean\",\n",
    "                   variables=  \"amount\",\n",
    "                   groupby_variables = \"nameOrig\"                         \n",
    "                 ) :\n",
    "        self.groupping_method = groupping_method\n",
    "        self.variables=variables\n",
    "        self.groupby_variables=groupby_variables\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Learn the mean or median of  amount of each client which will be used to create new feature for each unqiue client in order to undersatant thier behavior .\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: pandas dataframe of shape = [n_samples, n_features]\n",
    "        The training dataset. Can be the entire dataframe, not just the\n",
    "        variables to be transformed.\n",
    "        y: pandas Series, default = None\n",
    "        y is not needed in this encoder. You can pass y or None.\n",
    "        \"\"\"\n",
    "        X = X.copy()\n",
    "        self.group_amount_dict_ = {}\n",
    "        #df.groupby('card1')['TransactionAmt'].agg(['mean']).to_dict()\n",
    "        #temp = df.groupby('card1')['TransactionAmt'].agg(['mean']).rename({'mean':'TransactionAmt_card1_mean'},axis=1)\n",
    "        #df = pd.merge(df,temp,on='card1',how='left')\n",
    "        #target_mean = df_train.groupby(['id1', 'id2'])['target'].mean().rename('avg')\n",
    "        #df_test = df_test.join(target_mean, on=['id1', 'id2'])\n",
    "        #lifeExp_per_continent = gapminder.groupby('continent').lifeExp.mean()\n",
    "        # learn mean/medain \n",
    "        #for groupby in self.groupby_variables:\n",
    "         #   for var in self.variables:\n",
    "\n",
    "        print('we have {} unique clients'.format(X[self.groupby_variables].nunique()))\n",
    "        new_col_name =  self.variables+'_Transaction_'+ self.groupping_method    \n",
    "        X[new_col_name] = X.groupby([self.groupby_variables])[[self.variables]].transform(self.groupping_method)\n",
    "        X = X.drop_duplicates(['nameOrig'])\n",
    "    \n",
    "        self.group_amount_dict_ = dict(zip(X[self.groupby_variables], X[new_col_name]))\n",
    "        del X\n",
    "        #print('we have {} unique mean amount : one for each client'.format(len(self.group_amount_dict_)))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X = X.copy()\n",
    "        #for col in self.variables:\n",
    "         #   for agg_type in self.groupping_method:\n",
    "        new_col_name =  self.variables+'_Transaction_'+ self.groupping_method\n",
    "        X[new_col_name] = X[self.groupby_variables].map(self.group_amount_dict_)\n",
    "        return X[new_col_name].to_numpy().reshape(-1,1)   \n",
    "    \n",
    "############################################  \n",
    "class FeaturesEngineerCumCount(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,group_one =\"step\",\n",
    "                   group_two=  \"nameOrig\"                       \n",
    "                 ) :\n",
    "        self.group_one =group_one\n",
    "        self.group_two=group_two\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X = X.copy()\n",
    "        new_col_name =  self.group_two+'_Transaction_count'\n",
    "        X[new_col_name] = X.groupby([self.group_one, self.group_two])[[self.group_two]].transform('count')\n",
    "        return X[new_col_name].to_numpy().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete pipe :\n",
    "# select the float/cat columns\n",
    "#cat_feautres = X.select_dtypes(include=['object','category']).columns\n",
    "#num_features = X.select_dtypes(exclude=['object','category']).columns\n",
    "#Define vcat pipeline\n",
    "features_cum_count=['step','nameOrig']\n",
    "features_groupby_amount=['amount','nameOrig']\n",
    "features_frequency_orig_dest=['nameOrig','nameDest']\n",
    "features_cum_count_pipe = Pipeline([\n",
    "                     ('transformer_Encoder', FeaturesEngineerCumCount())\n",
    "                    ])\n",
    "features_groupby_pipe = Pipeline([\n",
    "                     ('transformer_group_amount_mean', FeaturesEngineerGroup2()),\n",
    "                     ('transformer_group_scaler', PowerTransformer())\n",
    "                    ])\n",
    "features_frequency_pipe = Pipeline([\n",
    "                     ('Encoder', CountFrequencyEncoder(variables=['nameOrig','nameDest'],encoding_method =\"frequency\", keep_variable=False))\n",
    "                    ])\n",
    "type_pipe= Pipeline([\n",
    "                     ('transformer_Encoder', ce.cat_boost.CatBoostEncoder())\n",
    "                    ])\n",
    "num_features0=[  'amount',  'oldbalanceOrig', 'newbalanceOrig' ,'oldbalanceDest', 'newbalanceDest']\n",
    "#Define vnum pipeline\n",
    "num_pipe = Pipeline([\n",
    "                     ('scaler', PowerTransformer()),\n",
    "                    ])\n",
    "#Featureunion fitting training data\n",
    "preprocessor = FeatureUnion(transformer_list=[('cum_count', features_cum_count_pipe),\n",
    "                                              ('mean_amount', features_groupby_pipe),\n",
    "                                              ('frequency_dest_orig', features_frequency_pipe),\n",
    "                                              ('trans_type', type_pipe),\n",
    "                                              ('num', num_pipe)])\n",
    "data_preparing= ColumnTransformer([\n",
    "    ('cum_count', features_cum_count_pipe, features_cum_count ),\n",
    "    ('mean_amount', features_groupby_pipe, features_groupby_amount ),\n",
    "    ('frequency_dest_orig', features_frequency_pipe, features_frequency_orig_dest ),\n",
    "    ('trans_type', type_pipe, ['type'] ),\n",
    "    ('num', num_pipe, num_features0)\n",
    "], remainder='drop')\n",
    "data_preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "current_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloder=DataLoader()\n",
    "data = dataloder.load_data(Config.from_json(cfg).data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required Python libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "#from configs_json import cfg\n",
    "#from utils.config import  Config\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#from dataloader.dataloader  import  DataLoader\n",
    "\n",
    "def create_train_test_data(dataset,train_size=0.5):\n",
    "    # load and split the data\n",
    "    data_train = dataset.sample(frac=train_size, random_state=30).reset_index(drop=True)\n",
    "    data_test = dataset.drop(data_train.index).reset_index(drop=True)\n",
    "    # save the data\n",
    "    data_train.to_csv('C:/Users/rzouga/Desktop/ALLINHERE/ALLINHERE/FraudDetection/DeployPipeComplet/data/train.csv', index=False)\n",
    "    data_test.to_csv('C:/Users/rzouga/Desktop/ALLINHERE/ALLINHERE/FraudDetection/DeployPipeComplet/data/test.csv', index=False)\n",
    "    print(f\"Train data for modeling: {data_train.shape}\")\n",
    "    print(f\"Test data for predictions: {data_test.shape}\")\n",
    "\n",
    "def train_model(x_train, y_train):\n",
    "     \n",
    "    print(\"Training the model ...\")\n",
    "    # Model defined \n",
    "    params={\"learning_rate\": Config.from_json(cfg).model.learning_rate,\n",
    "        #'device': Config.from_json(cfg).model.device,\n",
    "        'metric':Config.from_json(cfg).model.metric,\n",
    "        'objective':Config.from_json(cfg).model.objective,\n",
    "        'n_estimators':  Config.from_json(cfg).model.n_estimators,\n",
    "        'num_leaves': Config.from_json(cfg).model.num_leaves,\n",
    "        'min_child_samples':  Config.from_json(cfg).model.min_child_samples,\n",
    "        'feature_fraction': Config.from_json(cfg).model.feature_fraction,\n",
    "        'bagging_fraction': Config.from_json(cfg).model.bagging_fraction,\n",
    "        'bagging_freq':  Config.from_json(cfg).model.bagging_freq,\n",
    "        #'reg_alpha':  Config.from_json(cfg).model.reg_alpha,\n",
    "        #'reg_lambda':  Config.from_json(cfg).model.reg_lambda,\n",
    "       # 'gpu_platform_id':  Config.from_json(cfg).model.gpu_platform_id\n",
    "            }\n",
    "    params_optuna={'n_estimators': 11932, \n",
    "                    'max_depth': 16, \n",
    "                    'learning_rate': 0.005352340588475586,\n",
    "                    'lambda_l1': 1.4243404105489683e-06,\n",
    "                    'lambda_l2': 0.04777178032735788,\n",
    "                    'num_leaves': 141, \n",
    "                    'feature_fraction': 0.6657626611307914, \n",
    "                    'bagging_fraction': 0.9115997498937961,\n",
    "                    'bagging_freq': 1,\n",
    "                    'min_child_samples': 51,\n",
    "                     \"objective\": \"binary\",\n",
    "                     #\"metric\": \"binary_logloss\",\n",
    "                     \"verbosity\": -1,\n",
    "                     \"boosting_type\": \"gbdt\",\n",
    "                     #\"random_state\": 228,\n",
    "                     \"metric\": \"auc\",\n",
    "                     #\"device\": \"gpu\",\n",
    "                     'tree_method': \"gpu_hist\"\n",
    "                    }\n",
    "    model_lgbm  = LGBMRegressor(**params_optuna)\n",
    "    # Pipline preprocess\n",
    "    pipeline_model_lgbm = Pipeline([ \n",
    "        ('pre', data_preparing),\n",
    "        ('lgbm', model_lgbm)\n",
    "    ])\n",
    "    pipeline_model_lgbm.fit(x_train, y_train)\n",
    "\n",
    "    return pipeline_model_lgbm\n",
    "\n",
    "def accuracy(model, x_test, y_test):\n",
    "    print(\"Testing the model ...\")\n",
    "    predictions = model.predict(x_test)\n",
    "    roc_auc = roc_auc_score(y_test, predictions)\n",
    "    return roc_auc\n",
    "\n",
    "def export_model(model):\n",
    "    # Save the model\n",
    "    joblib_path = 'C:/Users/rzouga/Desktop/ALLINHERE/ALLINHERE/FraudDetection/DeployPipeComplet/models/model_test.joblib'\n",
    "    with open(joblib_path, 'wb') as file:\n",
    "        joblib.dump(model, file)\n",
    "        print(f\"Model saved at {joblib_path}\")\n",
    "\n",
    "def main():\n",
    "    #mlops data \n",
    "    # data =\n",
    "    # Load the whole data\n",
    "    dataloder=DataLoader()\n",
    "    data = dataloder.load_data(Config.from_json(cfg).data)\n",
    "  \n",
    "    # Split train/test\n",
    "    # Creates train.csv and test.csv\n",
    "    create_train_test_data(data.iloc[0:5000,:],train_size=0.5)\n",
    "    # Loads the data for the model training\n",
    "    train = pd.read_csv('C:/Users/rzouga/Desktop/ALLINHERE/ALLINHERE/FraudDetection/DeployPipeComplet/data/train.csv', keep_default_na=False)\n",
    "    #num_columns=train.drop(['isFraud'], axis=1).select_dtypes(include=['int64','float64']).columns\n",
    "    #cat_columns= train.drop(['isFraud'], axis=1).select_dtypes(exclude=['int64','float64']).columns\n",
    "    x_train = train.drop(['isFraud'], axis=1)\n",
    "    y_train =  train['isFraud'].to_numpy()\n",
    "     \n",
    "    # Loads the data for the model testing\n",
    "    test = pd.read_csv('C:/Users/rzouga/Desktop/ALLINHERE/ALLINHERE/FraudDetection/DeployPipeComplet/data/test.csv', keep_default_na=False)\n",
    "    x_test = test.drop(['isFraud'], axis=1)\n",
    "    y_test = test['isFraud'].to_numpy()\n",
    "\n",
    "    # Train and Test\n",
    "    model = train_model(x_train, y_train)\n",
    "    auc_test = accuracy(model, x_test, y_test)\n",
    "   \n",
    "    print(f\"auc_test: {auc_test}\")\n",
    "\n",
    "    # Save the model\n",
    "    export_model(model)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model data\n",
    "#f = \"C:/Users/rzouga/Desktop/ALLINHERE/ALLINHERE/FraudDetection/DeployPipeComplet/models/model_test.joblib\"\n",
    "f = \"C:/Users/rzouga/Desktop/ALLINHERE/ALLINHERE/FraudDetection/pipeline_model_lgbm_final_local.joblib\"\n",
    "model = joblib.load(f)\n",
    "item={}\n",
    "item={\"step\":234,\n",
    "       \"type\":\"CASH_OUT\",\n",
    "       \"amount\":305822.52,\n",
    "       \"nameOrig\":\"C1376293938\",\n",
    "       \"oldbalanceOrig\":0.0,\n",
    "       \"newbalanceOrig\":0.0,\n",
    "       \"nameDest\":\"C182325611\",\n",
    "       \"oldbalanceDest\":1569390.8999999999,\n",
    "       \"newbalanceDest\":1875213.4199999999}\n",
    "df = pd.json_normalize(item)\n",
    "df2 =pd.DataFrame([item])\n",
    "#data = test1.dict()\n",
    "#step=data['step']\n",
    "#type=data['type']\n",
    "#amount=data['amount']\n",
    "#nameOrig=data['nameOrig']\n",
    "#oldbalanceOrig=data['oldbalanceOrig']\n",
    "#newbalanceOrig=data['newbalanceOrig']\n",
    "#oldbalanceDest=data['oldbalanceDest']\n",
    "#newbalanceDest=data['newbalanceDest']\n",
    "# print(classifier.predict([[variance,skewness,curtosis,entropy]]))\n",
    "#prediction = classifier.predict([[variance,skewness,curtosis,entropy]])\n",
    "# Predicting Test Set\n",
    "test2={\"step\":item[\"step\"],\n",
    "       \"type\":item[\"type\"],\n",
    "       \"amount\":item[\"amount\"],\n",
    "       \"nameOrig\":item[\"nameOrig\"],\n",
    "       \"oldbalanceOrig\":item[\"oldbalanceOrig\"],\n",
    "       \"newbalanceOrig\":item[\"newbalanceOrig\"],\n",
    "       \"nameDest\":item[\"nameDest\"],\n",
    "       \"oldbalanceDest\":item[\"oldbalanceDest\"],\n",
    "       \"newbalanceDest\":item[\"newbalanceDest\"]}\n",
    "\n",
    "data_dict = pd.DataFrame(\n",
    "        {\n",
    "            'step': [item[\"step\"]],\n",
    "            'type': [item[\"type\"]],\n",
    "            'amount': [item[\"amount\"]],\n",
    "            'nameOrig': [item[\"nameOrig\"]],\n",
    "            'oldbalanceOrig': [item['oldbalanceOrig']],\n",
    "            'newbalanceOrig': [item[\"newbalanceOrig\"]],\n",
    "            'nameDest': [item[\"nameDest\"]],\n",
    "            'oldbalanceDest': [item[\"oldbalanceDest\"]],\n",
    "            'newbalanceDest': [item[\"newbalanceDest\"]]\n",
    "        }\n",
    "    )\n",
    "prediction = model.predict(data_dict)\n",
    "if (prediction[0] > 0.5):\n",
    "    prediction = \"Fraud\"\n",
    "else:\n",
    "    prediction = \"Not Fraud\"\n",
    "print(prediction)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we will need to import the library and initialize the main application object:\n",
    "import joblib\n",
    "import uvicorn\n",
    "from fastapi import FastAPI,Request, File, UploadFile, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import nest_asyncio\n",
    "from typing import Any, Dict,List\n",
    "\n",
    "        \n",
    "## API INSTANTIATION\n",
    "## ----------------------------------------------------------------\n",
    "       \n",
    "app = FastAPI(\n",
    "    title=\"Fraud Detection API\",\n",
    "    description=\"A simple API that use Ml model to predict fraud \",\n",
    "    version=\"0.1\",\n",
    ")\n",
    "# Creating the data model for data validation\n",
    "class ClientData(BaseModel):\n",
    "    step: List[int]\n",
    "    type: List[str]\n",
    "    amount: List[float]\n",
    "    nameOrig:  List[str]\n",
    "    oldbalanceOrig: List[float]\n",
    "    newbalanceOrig: List[float]\n",
    "    nameDest:  List[str]\n",
    "    oldbalanceDest: List[float]\n",
    "    newbalanceDest: List[float]\n",
    "\n",
    "#load model data\n",
    "f = \"C:/Users/rzouga/Desktop/ALLINHERE/ALLINHERE/FraudDetection/pipeline_model_lgbm_final_local.joblib\"\n",
    "#f = \"C:/Users/rzouga/Desktop/ALLINHERE/ALLINHERE/FraudDetection/DeployPipeComplet/models/pipeline_model_lgbm_final.joblib\"\n",
    "model = joblib.load(f)\n",
    "    \n",
    "## API ENDPOINTS\n",
    "## ----------------------------------------------------------------\n",
    "## API ENDPOINTS\n",
    "## ----------------------------------------------------------------\n",
    "\n",
    "##################\n",
    "@app.get('/')\n",
    "def index():\n",
    "  '''\n",
    "  This is a first docstring.\n",
    "  '''\n",
    "  return {'message': 'This is a Fraud  Classification API!'}\n",
    "\n",
    "# Tester\n",
    "@app.get('/ping')\n",
    "def ping():\n",
    "  '''\n",
    "  This is a first docstring.\n",
    "  '''\n",
    "  return ('pong', 200)\n",
    "# Defining the prediction endpoint without data validation\n",
    "@app.post('/basic_predict')\n",
    "async def basic_predict(request: Request):\n",
    "    '''\n",
    "    This is a first docstring.\n",
    "    '''\n",
    "    # Getting the JSON from the body of the request\n",
    "    input_data = await request.json()\n",
    "\n",
    "    # Converting JSON to Pandas DataFrame\n",
    "    input_df = pd.DataFrame([input_data])\n",
    "\n",
    "    # Getting the prediction \n",
    "    pred = model.predict(input_df)[0]\n",
    "\n",
    "    return pred\n",
    "\n",
    "# We now define the function that will be executed for each URL request and return the value:\n",
    "@app.post(\"/predict-fraud\")\n",
    "async  def predict_fraud(item :ClientData):\n",
    "    \"\"\"\n",
    "    A simple function that receive a client data and predict Fraud.\n",
    "    :param client_data:\n",
    "    :return: prediction, probabilities\n",
    "\n",
    "    \"\"\"\n",
    "    # perform prediction\n",
    "    #df =pd.DataFrame([item])\n",
    "    h=item.dict()\n",
    "    df=pd.DataFrame.from_dict(h, orient=\"columns\")\n",
    "    prediction = model.predict(df)\n",
    "    prediction_final=[\"Fraud\" if (x > 0.5) else \"Not Fraud\" for x in prediction ]\n",
    "    return prediction_final\n",
    "    \n",
    "    # Create the POST endpoint with path '/predict'\n",
    "@app.post(\"/predict_csv\")\n",
    "async def create_upload_file(file: UploadFile = File(...)):\n",
    "    # Handle the file only if it is a CSV\n",
    "    if file.filename.endswith(\".csv\"):\n",
    "        # Create a temporary file with the same name as the uploaded \n",
    "        # CSV file to load the data into a pandas Dataframe\n",
    "        with open(file.filename, \"wb\")as f:\n",
    "            f.write(file.file.read())\n",
    "        data = pd.read_csv(file.filename)\n",
    "        os.remove(file.filename)\n",
    "        # Return a JSON object containing the model predictions\n",
    "        return {\n",
    "            \"predictions\": model.predict(data)\n",
    "        }    \n",
    "    else:\n",
    "        # Raise a HTTP 400 Exception, indicating Bad Request \n",
    "        # (you can learn more about HTTP response status codes here)\n",
    "        raise HTTPException(status_code=400, detail=\"Invalid file format. Only CSV Files accepted.\")\n",
    "\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app, port=4000)\n",
    "# uvicorn app:app --reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item={\n",
    "  \"step\": [\n",
    "    0,234\n",
    "  ],\n",
    "  \"type\": [\n",
    "    \"string\",\"CASH_OUT\"\n",
    "  ],\n",
    "  \"amount\": [\n",
    "    0,305822.52\n",
    "  ],\n",
    "  \"nameOrig\": [\n",
    "    \"string\",\"C1376293938\"\n",
    "  ],\n",
    "  \"oldbalanceOrig\": [\n",
    "    0,0.0\n",
    "  ],\n",
    "  \"newbalanceOrig\": [\n",
    "    0,0.0\n",
    "  ],\n",
    "  \"nameDest\": [\n",
    "    \"string\",\"C182325611\"\n",
    "  ],\n",
    "  \"oldbalanceDest\": [\n",
    "    0,1569390.8999999999\n",
    "  ],\n",
    "  \"newbalanceDest\": [\n",
    "    0,1875213.4199999999\n",
    "  ]\n",
    "}\n",
    "df=pd.DataFrame.from_dict(item, orient=\"columns\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(df)\n",
    "prediction_final=[\"Fraud\" if (x > 0.5) else \"Not Fraud\" for x in prediction]\n",
    "prediction_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install session_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import session_info\n",
    "session_info.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
